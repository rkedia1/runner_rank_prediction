{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f683e97c-2855-4b1f-b0d2-b9510dde9b5d",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "In this course assignment you must build a predictive model to determine what place a runner will come in in a foot race. More specifically, you must predict the place order for all participants running races in the year 2019 which are part of a series (e.g. they have been run annually, or at least once previously) using historical data. **For each race you must predict the integer place ordering for all participants**. You are not predicting the top n finishers, or performance bands where people will finish. Instead, your method is expected to be integrated into a premium feature of an application such as Strava, where historical data and data about the racer (and who is signed up for the race!) could be used to help build a personalized prediction for them.\n",
    "\n",
    "## Framing\n",
    "Through this assignment you will demonstrate your ability to build sophisticated supervised machine learning models, from data manipulation through feature engineering and modelling. This is an authentic dataset, and a real-world problem. You can use whatever modelling method you would like to, and can characterise the problem as a regression, classification, or ordinal prediction problem. There is no particular guidelines you must follow, nor guidance offered in the course _per se_ however, there is plenty of opportunity to ask course staff questions. **It is expected that this assignment will take significant effort**.\n",
    "\n",
    "## About the Data\n",
    "All of the races you are asked to predict outcomes for have a temporal relationship with some race in the past (e.g. they are part of an annual series), and I have included an identifier `sequence_id` to help identify this. The `sequence_id` will be included in all races you need to predict, so you can build race-specific features should you wish to. Races which are in your training set and do not have a `sequence_id` could be used however you might like. There may be some races which have a `sequence_id` in the training set but do not have a `sequence_id` in the holdout set -- this all depends what is offered in a given year!\n",
    "\n",
    "A couple of core concepts are important beyond sequences. First, races have categories, which generally (though doesn't need to) denote the length of the race (e.g. 5k, marathon, etc). I've cleaned this column into a new one, prepending the word `clean` so that a columns such as `category.completed.name` becomes `clean_category.completed.name`. I have left the original data in there for you as well, and the transformations I've done have been largely to reduce dimensionality along lines I think is reasonable.\n",
    "\n",
    "In addition to a category, there are `brackets`. Brackets typical denote demographic aspects of the runners and group them, such as Men aged 40-45. I have removed bracket information from the data and instead want you to focus on overall prediction which merges all runners in a given category together. This (should) line up with the rank order based on the individual's time, though I have not verified it (and predicting time is **not** the task).\n",
    "\n",
    "## Evaluation Criteria\n",
    "In this assignment you will be penalized equally for incorrect predictions weighted by the distance by which you are incorrect within a given race. It does not matter whether you over or under predicted a given rank, you are penalized one point per position you are off for a given individual. All DNF's are removed from the dataset, so each person in the dataset has a rank. You *must* provide a predicted rank for each person however, you may rank multiple people at the same spot if you would like (e.g. ties). The evaluation is for each combination of event and category, so a given event may have a 5 kilometer category, a 1 mile category, and so forth. Only individuals registered for a given event and category combination are included in the `DataFrame` you will be asked to predict for. Each event/category pair is equally weighted, and is scaled by the size of the event. Your overall prediction score will be the sum of all scores across the prediction tasks (e.g. across unique combinations of event and category). The exact scoring function is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b18926-af2b-4b55-8dbb-f4af720df995",
   "metadata": {},
   "source": [
    "## Example Solution\n",
    "The following cell contains an example solution to demonstrate the API which is used for this assignment. In short, you are to create an `sklearn.pipeline.Pipeline` object which you `fit()` on your training data using whatever method you like and serialize it to disk in a file called `pipeline.cloudpickle`. This object will then be reinstantiated in the autograder and evaluated based on the scoring function described above. Please note that the solution below would be a poor one, it is intended **only** to demonstrate the API for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630de46-9e08-43a4-93e5-eefe0eec48a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# This is a custom transformer to demonstrate how you might modify the data for feature\n",
    "# selection or engineering before applying a given model. In this example I am only\n",
    "# doing feature selection, and passing to the next element in the pipeline the age\n",
    "# and bib number for the runner. Thus only two features will be used in my predictive model.\n",
    "# There are other ways to do this\n",
    "class CustomTransformer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # Just select the features we want\n",
    "        Xprime=X[['age','bib']]\n",
    "        # Ensure that they have numbers in them of the regression will fail\n",
    "        Xprime=Xprime.fillna(value=-1)\n",
    "        return Xprime\n",
    "\n",
    "# I build a very basic pipeline which is made up of three stages. In the first, my\n",
    "# custom transform is called and reduces the DataFrame to just two columns. In the\n",
    "# second I use a built-in transformer from sklearn to bucket users based on their\n",
    "# bib number, perhaps as a proxy for \"how early did they sign up\". In the final step\n",
    "# I want to use a LinearRegression() regressor.\n",
    "\n",
    "# There are two main concerns I need to address. First, I need to be resilient to bad\n",
    "# data which might address. So I know the LinearRegression() object can't handle\n",
    "# missing data, so I need to deal with that. This was done in the CustomeTransformer()\n",
    "# already.\n",
    "\n",
    "# Second, I actually need to be ranking results, not regressing. Depending upon your\n",
    "# model you need to consider this carefully. Here is a fine catch all if you\n",
    "# are using regression, and object which just ranks the results in order. This is\n",
    "# called monkey patching and replaces the LinearRegression() object's predict()\n",
    "# function with a wrapper\n",
    "reg=LinearRegression()\n",
    "reg.original_predict=reg.predict\n",
    "\n",
    "def new_predict(X):\n",
    "    # run the old regression method\n",
    "    rankings=reg.original_predict(X)\n",
    "    # now calculate and return the ranks of each item instead\n",
    "    # we need to add a +1 because the lowest rank is a 1, not a 0\n",
    "    # it's unfortunate, the first athletic competition was probably run by R users...\n",
    "    return rankings.squeeze().argsort()+1\n",
    "\n",
    "# Now we overwrite (monkey patch) the predict() function with our own implementation\n",
    "reg.predict=new_predict\n",
    "\n",
    "# And build our pipeline object\n",
    "pipe = make_pipeline( CustomTransformer(), QuantileTransformer(), reg )\n",
    "\n",
    "# This is just one way to do this, you could also implement a new estimator with the\n",
    "# predict interface and build all of your logic in there. The benefit of the\n",
    "# pipeline is that you can rapidly change the logic and try different pipelines using\n",
    "# common methods from sklearn. When the pipeline gets complicated, you can also\n",
    "# visualize it...\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "display(pipe)\n",
    "\n",
    "# Once the pipeline is built, we need to train it. I'm going to just do a pretty poor\n",
    "# job here, getting the training set provided\n",
    "df=pd.read_csv(\"../../assets/assignment/df_train.csv.gz\")\n",
    "\n",
    "# I'm just going to build a model off of one event/category combination (lame)\n",
    "training_data=df.query(\"`event.id`=='583f013a-1e54-4906-87f7-2b625206f5f9' and `clean_categories.name`=='5k'\")\n",
    "\n",
    "# And I'm going to pass in all of my potential columns for consideration. Note: The\n",
    "# example pipeline I built is going to reduce this to just the two columns I'm interested\n",
    "# in, so this is a safe thing to do. But be aware, the holdout set does not have all of\n",
    "# the data the training set might, because of leakage, so you need to think about this\n",
    "# and not make assumptions. You can see how I built the holdout set at the bottom of\n",
    "# this notebook\n",
    "X=set(training_data.columns)-{'overall_ranking'}\n",
    "\n",
    "# The ranking is what we aim to predict\n",
    "y={'overall_ranking'}\n",
    "\n",
    "# Now I fit() the pipeline. You'll note that the outcomes I need to squeeze() to ensure\n",
    "# it's a one dimensional structure and not a DataFrame\n",
    "fitted_pipe=pipe.fit(training_data[X],training_data[y].squeeze().to_numpy())\n",
    "\n",
    "# And now, assuming that I am happy with this model and think it is great, I write the\n",
    "# fitted pipeline to a file. This file will be read in by the autograder.\n",
    "cloudpickle.dump(fitted_pipe, open('pipeline.cloudpickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f295b8-719a-460b-aacf-2f8142a10664",
   "metadata": {},
   "source": [
    "## Testing the Solution\n",
    "With a minimum pipeline built we can think about testing it. The code below simulates the autograder, and is something you can use to evaluate how your model performs. The most important function is the `score()` function, which demonstrates how the score of the model fitness will be determined, as described previously. This function just compares two ranked lists and determined how aligned they are with one another. The second function is the `evaluate()` function, which runs your model over a given race of data. Note that the evaluation generates new ranks from the `overall_ranking` but doesn't use those numbers directly. Those numbers are in-order, but due to underlying data assumptions may have gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa10c44c-2a38-417e-a40b-80d77ee593ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "import sklearn\n",
    "\n",
    "# This code simulates the autograder. It is not the full autograder implementation\n",
    "# but shares an API with the autograder. It expects that your fitted pipeline is\n",
    "# submitted with the name pipeline.cloudpickle as demonstrated above. This object\n",
    "# must implement the predict() function. This is done automatically by the sklearn\n",
    "# Pipeline object if the last element of your pipeline is a classifier which has\n",
    "# a predict() function. If you are not submitting a Pipeline, and want to do something\n",
    "# different, you *must* have a predict() function of the same method signature, e.g.:\n",
    "#\n",
    "#   predict(self, X, **predict_params)->np.ndarray\n",
    "\n",
    "# Load holdout data, in this case I'll simulate it by loading the training data\n",
    "df=pd.read_csv(\"../../assets/assignment/df_train.csv.gz\")\n",
    "\n",
    "# And evaluate on all 5k races that we didn't consider for training\n",
    "holdout_data=df.query(\"`event.id`!='583f013a-1e54-4906-87f7-2b625206f5f9' and `clean_categories.name`=='5k'\")\n",
    "\n",
    "\n",
    "# This is the scoring function to determine model fitness\n",
    "def score(left: pd.DataFrame, right: pd.DataFrame):\n",
    "    '''\n",
    "    Calculates the difference between the left and the right when considering rank of items. \n",
    "    This scoring function requires that the two DataFrames have identical indicies, and that\n",
    "    they each contain only one column of values and no missing values. Props to Blake Atkinson\n",
    "    for providing MWE indicating issues with autograder version #1.\n",
    "    '''\n",
    "    assert(type(left)==pd.DataFrame)\n",
    "    assert(type(right)==pd.DataFrame)\n",
    "    assert(len(left)==len(right))\n",
    "    assert(not np.any(np.isnan(left)))\n",
    "    assert(not np.any(np.isnan(right)))\n",
    "    assert(left.index.equals(right.index))\n",
    "    # convert to ndarrays\n",
    "    left=left.squeeze()\n",
    "    right=right.squeeze()\n",
    "    return np.sum(np.abs(left-right))/(len(left)*(len(left)-1))\n",
    "\n",
    "# This function runs the prediction model agains a given event/category pair. It\n",
    "# intentionally loads the student model each time to avoid accidental leakage of data\n",
    "# between events.\n",
    "def evaluate(data, pipeline_file='pipeline.cloudpickle'):\n",
    "    # Load student pipeline\n",
    "    fitted_pipe = cloudpickle.load(open(pipeline_file,'rb'))\n",
    "    \n",
    "    # Separate out the X and y\n",
    "    X=list(set(data.columns)-{'overall_ranking'})\n",
    "    y=['overall_ranking']\n",
    "    \n",
    "    # Drop any missing results (DNFs)\n",
    "    data=data.dropna(subset=['overall_ranking'])\n",
    "    \n",
    "    # Ensure there is data to actually predict on\n",
    "    if len(data)==0:\n",
    "        return np.nan\n",
    "\n",
    "    # Predict on unseen data\n",
    "    predictions=pd.DataFrame(fitted_pipe.predict(data[X]),data.index)\n",
    "    observed=data[y]\n",
    "    \n",
    "    # Generate rankings within this bracket\n",
    "    observed=pd.DataFrame(data[y].rank(),data.index)\n",
    "    \n",
    "    # Return the ratio of the student score\n",
    "    return pd.Series({\"score\":score(observed,predictions)})\n",
    "\n",
    "# Student solution\n",
    "pipeline_file='pipeline.cloudpickle'\n",
    "\n",
    "# Run prediction on each group\n",
    "results=holdout_data.groupby([\"event.id\",\"clean_categories.name\"]).apply(evaluate, pipeline_file)\n",
    "\n",
    "# Display the results, uncomment this for your own display\n",
    "results.reset_index()['score'].plot.bar();\n",
    "\n",
    "# This is the student final grade\n",
    "print(np.average(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
